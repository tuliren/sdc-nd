{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "- Gradient descent\n",
    "  - Descend a mountain\n",
    "  - Descend in the direction that will decrease the error the most\n",
    "  - Until a minimum error is found\n",
    "- Least squares\n",
    "  - error = (prediction - actual)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear to logistic regression\n",
    "\n",
    "- Linear regression\n",
    "  - Predict values on a continuous spectrum\n",
    "\n",
    "- Logistic regression\n",
    "  - Classify data among discrete classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "- Chain logistic regression layers together\n",
    "  - Input layer\n",
    "  - Hidden layer\n",
    "  - Output layer\n",
    "  \n",
    "  <img src=\"images/perceptron.jpg\" width=\"50%\" height=\"50%\" />\n",
    "  \n",
    "- Perceptron\n",
    "  - Articial neuron\n",
    "  - Each one looks at input data and decides how to categorize that data.\n",
    "  - Output is always 0 or 1.\n",
    "- Weights\n",
    "  - Input is multipled by a weight value\n",
    "  - Start as random values\n",
    "  - Neural network is trained by adjusting weights\n",
    "  - Higher weight means that this input is more important than other inputs\n",
    "  - Weighted input values are summed to a single value\n",
    "  - Matrix of weights: $W$\n",
    "  - Individual weight: $w$\n",
    "- Activation function\n",
    "  - Result of perceptron's summation is turned into output signal by activation function\n",
    "\n",
    "  - Heaviside step function\n",
    "    - `f(h) = h >= 0 ? 1 : 0`\n",
    "    <img src=\"images/heaviside-step-function.png\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "- Bias: $b$, moves values in one direction or another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def test(weight1, weight2, bias, test_inputs, correct_outputs):    \n",
    "    outputs = []\n",
    "\n",
    "    # Generate and check output\n",
    "    for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "        linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "        output = int(linear_combination >= 0)\n",
    "        is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "        outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "    # Print output\n",
    "    num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "    output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "    if not num_wrong:\n",
    "        print('Nice!  You got it all correct.\\n')\n",
    "    else:\n",
    "        print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "\n",
    "    print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND perceptron\n",
    "<img src=\"images/and-perceptron.png\" width=\"30%\" height=\"30%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                   -10                    0          Yes\n",
      "      0          1                    -5                    0          Yes\n",
      "      1          0                    -5                    0          Yes\n",
      "      1          1                     0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "weight1 = 5\n",
    "weight2 = 5\n",
    "bias = -10\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "\n",
    "test(weight1, weight2, bias, test_inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                    -5                    0          Yes\n",
      "      0          1                     0                    1          Yes\n",
      "      1          0                     0                    1          Yes\n",
      "      1          1                     5                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "weight1 = 5\n",
    "weight2 = 5\n",
    "bias = -5\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, True, True, True]\n",
    "\n",
    "test(weight1, weight2, bias, test_inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                     0                    1          Yes\n",
      "      0          1                    -1                    0          Yes\n",
      "      1          0                     0                    1          Yes\n",
      "      1          1                    -1                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "weight1 = 0\n",
    "weight2 = -1\n",
    "bias = 0\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "test(weight1, weight2, bias, test_inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR perceptron\n",
    "<img src=\"images/xor-perceptron.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "- A: `NOT`\n",
    "- B: `AND`\n",
    "- C: `OR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "<img src=\"images/simple-network.png\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "- Use activation functions that are continuous and differentiable, possible to train using gradient descent\n",
    "\n",
    "- Logistic (sigmoid) activation function\n",
    "  - $sigmoid(x) = 1 / (1 + e^{-x})$\n",
    "  - Output $(0, 1)$\n",
    "  - Can be interpreted as a probability for success.\n",
    "  - Same formulation as logistic regression.\n",
    "  - Turn perceptron into neural network.\n",
    "  <img src=\"images/sigmoid.png\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "- Circles: units\n",
    "- Boxes: operations\n",
    "\n",
    "- $h = \\sum w_i x_i + b$\n",
    "- $y = f(h)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "0.432907095035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calculate the output\n",
    "output = sigmoid(np.dot(weights, inputs) + bias)\n",
    "\n",
    "print('Output:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "- Sum of square errors (SSE)\n",
    "  - Metric of how wrong the predictions are\n",
    "  - $E = \\frac{1}{2}\\sum_\\mu\\sum_j[y^\\mu_j - \\hat{y}^\\mu_j]^2$\n",
    "    - $\\hat{y}$ - prediction\n",
    "    - $y$ - true value\n",
    "    - $j$ - output units\n",
    "    - $\\mu$ - data points\n",
    "    - First sum over $j$, then over $\\mu$\n",
    "  - $\\hat{y}^\\mu_j = f(\\sum_i w_{ij} x^\\mu_i)$\n",
    "  - Find weights $w_{ij}$ that minimize the squared error $E$, using **gradient descent**.\n",
    "\n",
    "- Mean of square errors (MSE)\n",
    "  - If a lot of data is used, summing up all the weight steps can lead to really large updates that make the gradient descent diverge.\n",
    "  - Error is divided by the number of records, $m$.\n",
    "  - $E = \\frac{1}{2m}\\sum_\\mu(y^\\mu-\\hat{y}^mu)^2$\n",
    "\n",
    "- Gradient descent\n",
    "  - Gradient: rate of change; slope; a derivative generalized to function with more than one variable.\n",
    "  - Calculate the error and the gradient, and change each weight in the direction of the largest gradient.\n",
    "  - $\\Delta{w} = - gradient$\n",
    "  - $w_i = w_i + \\Delta{w_i}$\n",
    "  - $\\Delta{w_i} \\propto -\\frac{\\partial{E}}{\\partial{w_i}}$ -- gradient\n",
    "  - $\\Delta{w_i} = - \\eta \\frac{\\partial{E}}{\\partial{w_i}}$ -- add an arbitrary scaling parameter, learning rate $\\eta$\n",
    "    <img src=\"images/gradient-descent.png\" width=\"30%\" height=\"30%\"/>\n",
    "\n",
    "  - $\\frac{\\partial{E}}{\\partial{w_i}} = -(y - \\hat{y})f^\\prime(h)x_i$\n",
    "  - $\\Delta{w_i} = \\eta(y - \\hat{y})f^\\prime(h)x_i$\n",
    "  - $= learning\\_rate \\times error \\times activate\\_derivative \\times input$\n",
    "  - Error term $\\delta = (y - \\hat{y})f^\\prime(h)$\n",
    "  - $w_i = w_i + \\eta \\delta x_i$\n",
    "    <img src=\"images/network-calculation.jpg\" width=\"50%\" height=\"50%\"/>\n",
    "\n",
    "- Caveats: when weights are initialized with wrong values, gradient descent could lead weights into local minimum, but not global minimum.\n",
    "  - Solution: [momentum](http://ruder.io/optimizing-gradient-descent/index.html#momentum) helps accelerate gradient descent in the relevant direction and dampens oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.377540668798\n",
      "Amount of Error:\n",
      "0.122459331202\n",
      "Change in Weights:\n",
      "[ 0.0143892  0.0287784]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([1, 2])\n",
    "# Target\n",
    "y = np.array(0.5)\n",
    "# Input to output weights\n",
    "weights = np.array([0.5, -0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "h = np.dot(x, weights)\n",
    "y_hat = sigmoid(h)\n",
    "error = y - y_hat\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(y_hat)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "- Set weight step to zero $$\\Delta{w}_i = 0$$\n",
    "- For each record in training data:\n",
    "  - Make a forward pass through the network and calculate output unit $$h = \\sum_i w_i x_i$$\n",
    "  - Apply activation function and get the output $$\\hat{y} = f(h)$$\n",
    "  - Calculate the error gradient in the output $$\\delta = (y - \\hat{y}) \\times f^\\prime(h)$$\n",
    "  - Update weight step $$\\Delta{w_i} = \\Delta{w_i} + \\delta x_i$$\n",
    "  - Final $\\Delta w_i$ is the summed weight step across all inputs\n",
    "- Update weights $$w_i = w_i + \\eta \\Delta w_i / m$$\n",
    "- Repeat for $e$ epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.256060280051\n",
      "Train loss:  0.199324509463\n",
      "Train loss:  0.197636657787\n",
      "Train loss:  0.197270122133\n",
      "Train loss:  0.197152222857\n",
      "Train loss:  0.19710738054\n",
      "Train loss:  0.197089215266\n",
      "Train loss:  0.197081981947\n",
      "Train loss:  0.197079469107\n",
      "Train loss:  0.197079000687\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "        h = np.dot(x, weights)\n",
    "        output = sigmoid(h)\n",
    "        output_prime = output / (1.0 - output)\n",
    "        error = (y - output) * output_prime\n",
    "\n",
    "        del_w += error * x\n",
    "\n",
    "    # Update weights\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptrons\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
