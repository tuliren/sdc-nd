{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "- Gradient descent\n",
    "  - Descend a mountain\n",
    "  - Descend in the direction that will decrease the error the most\n",
    "  - Until a minimum error is found\n",
    "- Least squares\n",
    "  - error = (prediction - actual)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear to logistic regression\n",
    "\n",
    "- Linear regression\n",
    "  - Predict values on a continuous spectrum\n",
    "\n",
    "- Logistic regression\n",
    "  - Classify data among discrete classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "- Chain logistic regression layers together\n",
    "  - Input layer\n",
    "  - Hidden layer - linear function\n",
    "  - Output layer - step function that outputs 0 or 1\n",
    "  \n",
    "  <img src=\"images/perceptron.jpg\" width=\"50%\" height=\"50%\" />\n",
    "  \n",
    "- Perceptron\n",
    "  - Articial neuron\n",
    "  - Each one looks at input data and decides how to categorize that data.\n",
    "  - Output is always 0 or 1.\n",
    "- Weights\n",
    "  - Input is multipled by a weight value\n",
    "  - Start as random values\n",
    "  - Neural network is trained by adjusting weights\n",
    "  - Higher weight means that this input is more important than other inputs\n",
    "  - Weighted input values are summed to a single value\n",
    "  - Matrix of weights: $W$\n",
    "  - Individual weight: $w$\n",
    "- Activation function\n",
    "  - Result of perceptron's summation is turned into output signal by activation function\n",
    "\n",
    "  - Heaviside step function\n",
    "    - `f(h) = h >= 0 ? 1 : 0`\n",
    "    <img src=\"images/heaviside-step-function.png\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "- Bias: $b$, moves values in one direction or another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def test(weight1, weight2, bias, test_inputs, correct_outputs):    \n",
    "    outputs = []\n",
    "\n",
    "    # Generate and check output\n",
    "    for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "        linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "        output = int(linear_combination >= 0)\n",
    "        is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "        outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "    # Print output\n",
    "    num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "    output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "    if not num_wrong:\n",
    "        print('Nice!  You got it all correct.\\n')\n",
    "    else:\n",
    "        print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "\n",
    "    print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND perceptron\n",
    "<img src=\"images/and-perceptron.png\" width=\"30%\" height=\"30%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                   -10                    0          Yes\n",
      "      0          1                    -5                    0          Yes\n",
      "      1          0                    -5                    0          Yes\n",
      "      1          1                     0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "weight1 = 5\n",
    "weight2 = 5\n",
    "bias = -10\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "\n",
    "test(weight1, weight2, bias, test_inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                    -5                    0          Yes\n",
      "      0          1                     0                    1          Yes\n",
      "      1          0                     0                    1          Yes\n",
      "      1          1                     5                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "weight1 = 5\n",
    "weight2 = 5\n",
    "bias = -5\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, True, True, True]\n",
    "\n",
    "test(weight1, weight2, bias, test_inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                     0                    1          Yes\n",
      "      0          1                    -1                    0          Yes\n",
      "      1          0                     0                    1          Yes\n",
      "      1          1                    -1                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "weight1 = 0\n",
    "weight2 = -1\n",
    "bias = 0\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "test(weight1, weight2, bias, test_inputs, correct_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR perceptron\n",
    "<img src=\"images/xor-perceptron.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "- A: `NOT`\n",
    "- B: `AND`\n",
    "- C: `OR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Algorithm\n",
    "For a point with coordinates (p,q) , label y, and prediction given by the equation:\n",
    "\n",
    "$\\hat{y} = step(w_1 x_1 + w_2 x_2 + b)$\n",
    "\n",
    "- If the point is correctly classified, do nothing.\n",
    "- If the point is classified positive, but it has a negative label:\n",
    "  - $ w_1 = w_1 - \\alpha p $\n",
    "  - $ w_2 = w_2 - \\alpha q $\n",
    "  - $ b = b - \\alpha $\n",
    "- If the point is classified negative, but it has a positive label:\n",
    "  - $ w_1 = w_1 + \\alpha p $\n",
    "  - $ w_2 = w_2 + \\alpha q $\n",
    "  - $ b = b + \\alpha $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "<img src=\"images/simple-network.png\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "- Use activation functions that are continuous and differentiable, possible to train using gradient descent\n",
    "\n",
    "- Logistic (sigmoid) activation function\n",
    "  - $sigmoid(x) = 1 / (1 + e^{-x})$\n",
    "  - Output $(0, 1)$\n",
    "  - Can be interpreted as a probability for success.\n",
    "  - Same formulation as logistic regression.\n",
    "  - Turn perceptron into neural network.\n",
    "  <img src=\"images/sigmoid.png\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "- Circles: units\n",
    "- Boxes: operations\n",
    "\n",
    "- $h = \\sum w_i x_i + b$\n",
    "- $y = f(h)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "0.432907095035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calculate the output\n",
    "output = sigmoid(np.dot(weights, inputs) + bias)\n",
    "\n",
    "print('Output:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "- Sigmoid activation function for more than two classes\n",
    "- $P(x_k) = \\frac{e^{x_k}}{\\sum_i {e^{x_i}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.032058603280084988, 0.087144318742032573, 0.23688281808991013, 0.64391425988797224]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    total = np.sum(np.exp(L))\n",
    "    return list(map(lambda i: np.exp(i) / total, L))\n",
    "\n",
    "print(softmax([1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding\n",
    "\n",
    "- A process by which categorical variables are converted into a group of bits. For each category, only one bit within this group is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Method\n",
    "\n",
    "- Pick a model that gives existing labels the highest probability.\n",
    "  - Calculate the probability of each data point carrying the label according to the model;\n",
    "  - Multiple these probabilities to obtain the probability of the whole arrangement;\n",
    "  - Check which model is better.\n",
    "- Product is hard to use\n",
    "  - Multiplication of small values leads to tiny output\n",
    "  - Change in one value affects the output a lot\n",
    "  - Soluction: Turn product into sum\n",
    "\n",
    "## Cross-Entropy\n",
    "- Sum of negative logarithms of probabilities\n",
    "- The lower the better\n",
    "- Likely events with higher probabilities will have smaller cross entropy\n",
    "- $CrossEntropy = - \\sum_i{y_i \\ln{(p_i)} + (1 - y_i) ln{(1 - p_i)}}$\n",
    "- Tells whether two vectors are similar to each other\n",
    "  - $CE[(1, 1, 0), (0.8, 0.7, 0.1)] = 0.69$\n",
    "  - $CE[(0, 0, 1), (0.8, 0.7, 0.1)] = 5.12$\n",
    "  - $(1, 1, 0)$ is more similar to $(0.8, 0.7, 0.1)$ than $(0, 0, 1)$\n",
    "\n",
    "## Multi-Class Cross-Entropy\n",
    "$$CrossEntroy = - \\sum_{i=1}^{n}{\\sum_{j=1}^{m}{y_{ij} ln{(p_{ij})}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "- Sum of square errors (SSE)\n",
    "  - Metric of how wrong the predictions are\n",
    "  - $E = \\frac{1}{2}\\sum_\\mu\\sum_j[y^\\mu_j - \\hat{y}^\\mu_j]^2$\n",
    "    - $\\hat{y}$ - prediction\n",
    "    - $y$ - true value\n",
    "    - $j$ - output units\n",
    "    - $\\mu$ - data points\n",
    "    - First sum over $j$, then over $\\mu$\n",
    "  - $\\hat{y}^\\mu_j = f(\\sum_i w_{ij} x^\\mu_i)$\n",
    "  - Find weights $w_{ij}$ that minimize the squared error $E$, using **gradient descent**.\n",
    "\n",
    "- Mean of square errors (MSE)\n",
    "  - If a lot of data is used, summing up all the weight steps can lead to really large updates that make the gradient descent diverge.\n",
    "  - Error is divided by the number of records, $m$.\n",
    "  - $E = \\frac{1}{2m}\\sum_\\mu(y^\\mu-\\hat{y}^mu)^2$\n",
    "\n",
    "- Gradient descent\n",
    "  - Error function requirements\n",
    "    - Continuous\n",
    "    - Differentiable\n",
    "  - Gradient: rate of change; slope; a derivative generalized to function with more than one variable.\n",
    "  - Calculate the error and the gradient, and change each weight in the direction of the largest gradient.\n",
    "  - $\\Delta{w} = - gradient$\n",
    "  - $w_i = w_i - gradient = w_i + \\Delta{w_i}$\n",
    "  - $\\Delta{w_i} \\propto -\\frac{\\partial{E}}{\\partial{w_i}}$ -- gradient\n",
    "  - $\\Delta{w_i} = - \\eta \\frac{\\partial{E}}{\\partial{w_i}}$ -- add an arbitrary scaling parameter, learning rate $\\eta$\n",
    "    <img src=\"images/gradient-descent.png\" width=\"30%\" height=\"30%\"/>\n",
    "\n",
    "  - $\\frac{\\partial{E}}{\\partial{w_i}} = -(y - \\hat{y})f^\\prime(h)x_i$\n",
    "  - $\\Delta{w_i} = \\eta(y - \\hat{y})f^\\prime(h)x_i$\n",
    "  - $= learning\\_rate \\times error \\times activate\\_derivative \\times input$\n",
    "  - Error term $\\delta = (y - \\hat{y})f^\\prime(h)$\n",
    "  - $w_i = w_i + \\eta \\delta x_i$\n",
    "    <img src=\"images/network-calculation.jpg\" width=\"50%\" height=\"50%\"/>\n",
    "\n",
    "- Caveats: when weights are initialized with wrong values, gradient descent could lead weights into local minimum, but not global minimum.\n",
    "  - Solution: [momentum](http://ruder.io/optimizing-gradient-descent/index.html#momentum) helps accelerate gradient descent in the relevant direction and dampens oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.377540668798\n",
      "Amount of Error:\n",
      "0.122459331202\n",
      "Change in Weights:\n",
      "[ 0.0143892  0.0287784]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([1, 2])\n",
    "# Target\n",
    "y = np.array(0.5)\n",
    "# Input to output weights\n",
    "weights = np.array([0.5, -0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "h = np.dot(x, weights)\n",
    "y_hat = sigmoid(h)\n",
    "error = y - y_hat\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(y_hat)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "- Set weight step to zero $$\\Delta{w}_i = 0$$\n",
    "- For each record in training data:\n",
    "  - Make a forward pass through the network and calculate output unit $$h = \\sum_i w_i x_i$$\n",
    "  - Apply activation function and get the output $$\\hat{y} = f(h)$$\n",
    "  - Calculate the error gradient in the output $$\\delta = (y - \\hat{y}) \\times f^\\prime(h)$$\n",
    "  - Update weight step $$\\Delta{w_i} = \\Delta{w_i} + \\delta x_i$$\n",
    "  - Final $\\Delta w_i$ is the summed weight step across all inputs\n",
    "- Update weights $$w_i = w_i + \\eta \\Delta w_i / m$$\n",
    "- Repeat for $e$ epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.256060280051\n",
      "Train loss:  0.199324509463\n",
      "Train loss:  0.197636657787\n",
      "Train loss:  0.197270122133\n",
      "Train loss:  0.197152222857\n",
      "Train loss:  0.19710738054\n",
      "Train loss:  0.197089215266\n",
      "Train loss:  0.197081981947\n",
      "Train loss:  0.197079469107\n",
      "Train loss:  0.197079000687\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "        h = np.dot(x, weights)\n",
    "        output = sigmoid(h)\n",
    "        output_prime = output / (1.0 - output)\n",
    "        error = (y - output) * output_prime\n",
    "\n",
    "        del_w += error * x\n",
    "\n",
    "    # Update weights\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron vs Gradient Descent\n",
    "\n",
    "- Gradient descent\n",
    "  - Change $w_i$ to $w_i + \\alpha (y - \\hat{y})x_i$\n",
    "- Perceptron algorithm\n",
    "  - If correctly classified: $y - \\hat{y} = 0$\n",
    "  - If missclassified\n",
    "    - $y - \\hat{y} = 1$ **if positive**\n",
    "    - $y - \\hat{y} = -1$ if negative\n",
    "    - Change $w_i$ to\n",
    "      - $w_i + \\alpha x_i$ **if positive**\n",
    "      - $w_i - \\alpha x_i$ if netagive\n",
    "\n",
    "<img src=\"images/perceptron-vs-gradient-descent.png\" width=\"60%\" height=\"60%\"/>\n",
    "\n",
    "- When correctly classified, perceptron does nothing, but in gradient descent, the line will go farther away from the data point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture\n",
    "\n",
    "## Combination of neural networks\n",
    "\n",
    "<img src=\"images/combination-of-neural-networks-1.png\" width=\"60%\" height=\"60%\"/>\n",
    "\n",
    "<img src=\"images/combination-of-neural-networks-2.png\" width=\"60%\" height=\"60%\"/>\n",
    "\n",
    "## Multiple layers\n",
    "- Add more nodes to the input, hidden and output layers\n",
    "- Add more layers\n",
    "\n",
    "<img src=\"images/multilayers.png\" width=\"60%\" height=\"60%\"/>\n",
    "\n",
    "## Multi-class classification\n",
    "- Add more nodes to the output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "  - The process that neural networks use to turn the input into an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986]\n",
      "Input to hidden weights:\n",
      "[[-0.02341534 -0.0234137   0.15792128]\n",
      " [ 0.07674347 -0.04694744  0.054256  ]\n",
      " [-0.04634177 -0.04657298  0.02419623]\n",
      " [-0.19132802 -0.17249178 -0.05622875]]\n",
      "Hidden to output weights:\n",
      "[[-0.10128311  0.03142473]\n",
      " [-0.09080241 -0.14123037]\n",
      " [ 0.14656488 -0.02257763]]\n",
      "Hidden-layer Output:\n",
      "[ 0.41492192  0.42604313  0.5002434 ]\n",
      "Output-layer Output:\n",
      "[ 0.49815196  0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "# 1x4\n",
    "X = np.random.randn(4)\n",
    "\n",
    "print('Input:')\n",
    "print(X)\n",
    "\n",
    "# 4x3\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "# 3x2\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "print('Input to hidden weights:')\n",
    "print(weights_input_to_hidden)\n",
    "print('Hidden to output weights:')\n",
    "print(weights_hidden_to_output)\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "# 1x4 - 4x3 -> 1x3\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "# 1x3 - 3x2 -> 1x2\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "- Doing a feedforward operation.\n",
    "- Comparing the output of the model with the desired output.\n",
    "- Calculating the error.\n",
    "- Running the feedforward operation backwards (**backpropagation**) to spread the error to each of the weights.\n",
    "- Use this to update the weights, and get a better model.\n",
    "- Continue this until we have a model that is good.\n",
    "\n",
    "<img src=\"images/backpropagation-1.png\" width=\"60%\" height=\"60%\"/>\n",
    "\n",
    "- When a linear model correctly classifies a data point\n",
    "  - Increase the weight of that linear model\n",
    "  - Move the line closer to the data point\n",
    "- When a linear model incorrectly classifies a data point\n",
    "  - Decrease the weight of that linear model\n",
    "  - Move the line farther to the data point\n",
    "\n",
    "<img src=\"images/backpropagation-2.png\" width=\"60%\" height=\"60%\"/>\n",
    "\n",
    "- Feedforward: composing a bunch of functions.\n",
    "- Backpropagation: taking the derivative of a composition, which is multiplying a bund of derivatives (chain rule)\n",
    "\n",
    "<img src=\"images/feedforward-calculation.png\" width=\"60%\" height=\"60%\"/>\n",
    "<img src=\"images/backpropagation-calculation.png\" width=\"60%\" height=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "- Set weights for each layer to zero\n",
    "  - Input to hidden weights $\\Delta{w}_{ij} = 0$\n",
    "  - Hidden to output weights $\\Delta{W_{j} = 0}$\n",
    "- For each record in training data:\n",
    "  - Make a forward pass through the network and calculate output $\\hat{y}$\n",
    "  - Calculate the error gradient in the output\n",
    "    $$\\delta^0 = (y - \\hat{y}) \\times f^\\prime(z)$$\n",
    "    $$z = \\sum_j W_j a_j$$\n",
    "  - Propagate the errors to hidden layer\n",
    "    $$\\delta_j^h = \\delta^0 W_j f^\\prime(h_j)$$\n",
    "  \n",
    "  - Update weight steps\n",
    "    $$\\Delta{W_j} = \\Delta{W_j} + \\delta^0 a_j$$\n",
    "    $$\\Delta{w_{ij}} = \\Delta{w_{ij}} + \\delta_j^h a_i$$\n",
    "- Update weights\n",
    "  $$W_j = W_j + \\eta \\Delta W_j / m$$\n",
    "  $$w_{ij} = w_{ij} + \\eta \\Delta w_{ij} / m$$\n",
    "- Repeat for $e$ epoches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.229558087439\n",
      "Train loss:  0.229405884821\n",
      "Train loss:  0.229256817163\n",
      "Train loss:  0.229110801711\n",
      "Train loss:  0.22896775824\n",
      "Train loss:  0.228827608971\n",
      "Train loss:  0.228690278489\n",
      "Train loss:  0.228555693665\n",
      "Train loss:  0.228423783578\n",
      "Train loss:  0.228294479446\n",
      "Prediction accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output_input = np.dot(hidden_output, weights_hidden_output)\n",
    "        output_output = sigmoid(output_input)\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output_output\n",
    "\n",
    "        # TODO: Calculate error gradient in output unit\n",
    "        output_error = error * output_output * (1 - output_output)\n",
    "\n",
    "        # TODO: propagate errors to hidden layer\n",
    "        hidden_error = np.dot(output_error, weights_hidden_output) * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error * hidden_output\n",
    "        del_w_input_hidden += hidden_error * x[:, None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "- [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)\n",
    "  - Backpropagation is a leaky abstraction, and easily to fall into traps.\n",
    "    - Vanishing gradients on sigmoids\n",
    "    - Dying ReLUs\n",
    "    - Exploding gradients in RNNs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
